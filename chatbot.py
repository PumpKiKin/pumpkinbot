## streamlit ëª¨ë“ˆ ë¶ˆëŸ¬ì˜¤ê¸°
import streamlit as st
from streamlit.runtime.uploaded_file_manager import UploadedFile

from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.documents.base import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.prompts import PromptTemplate
from langchain_core.runnables import Runnable
from langchain.schema.output_parser import StrOutputParser
from typing import List
import os
import re
import json

## í™˜ê²½ë³€ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸°
from dotenv import load_dotenv,dotenv_values
load_dotenv()

from notice_crawler import show_notices

############################### 0ë‹¨ê³„ : ëŒ€í™” ë§¥ë½ ìœ ì§€ ê´€ë ¨ HISTORY í•¨ìˆ˜ë“¤ ##########################
# ì„¸ì…˜ íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”
if "chat_history" not in st.session_state:
    # role: "user" | "assistant"
    st.session_state.chat_history = []
if "summary" not in st.session_state:
    st.session_state.summary = ""   # (ì„ íƒ) ìš”ì•½ ë²„í¼

# ë¦¬ì…‹ ë²„íŠ¼
st.sidebar.button("ëŒ€í™” ì´ˆê¸°í™”", on_click=lambda: (st.session_state.update(chat_history=[]), st.session_state.update(summary="")))

# íˆìŠ¤í† ë¦¬ í¬ë§·í„°
def format_history_for_prompt(history, window_size=8):
    """í”„ë¡¬í”„íŠ¸ì— ë„£ì„ ìˆ˜ ìˆê²Œ ê°„ë‹¨ ë¬¸ìì—´ë¡œ ì •ë¦¬"""
    recent = history[-window_size:]
    lines = []
    for m in recent:
        prefix = "ì‚¬ìš©ì" if m["role"] == "user" else "ì–´ì‹œìŠ¤í„´íŠ¸"
        lines.append(f"{prefix}: {m['content']}")
    return "\n".join(lines)



############################### 1ë‹¨ê³„ : JSON ë¬¸ì„œë¥¼ ë²¡í„°DBì— ì €ì¥í•˜ëŠ” í•¨ìˆ˜ë“¤ ##########################

## 1: ì„ì‹œí´ë”ì— íŒŒì¼ ì €ì¥, ì´ì œ ì´ í•¨ìˆ˜ëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ(ë°ì´í„° ì—…ë¡œë“œ X, ë‚´ì¥ O)
def save_uploadedfile(uploadedfile: UploadedFile) -> str : 
    temp_dir = "JSON_ì„ì‹œí´ë”"
    if not os.path.exists(temp_dir):
        os.makedirs(temp_dir)
    file_path = os.path.join(temp_dir, uploadedfile.name)
    with open(file_path, "wb") as f:
        f.write(uploadedfile.read()) 
    return file_path

## 2: ì €ì¥ëœ JSON íŒŒì¼ì„ Documentë¡œ ë³€í™˜
def json_to_documents(json_path:str) -> List[Document]:
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)

    documents = []
    for i, item in enumerate(data):
        content = item.get("description", "")
        
         # list ì²˜ë¦¬: ë‚´ë¶€ì— dictê°€ ìˆëŠ” ê²½ìš°ë¥¼ í¬í•¨í•˜ì—¬ ë¬¸ìì—´ë¡œ ë³€í™˜
        if isinstance(content, list):
            content = "\n".join(
                [elem if isinstance(elem, str) else json.dumps(elem, ensure_ascii=False) for elem in content]
            )
        # dice ì²˜ë¦¬: key: value í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        elif isinstance(content, dict):
            content = "\n".join(f"{key}: {value}" for key, value in content.items())

        # metadata ë¶ˆëŸ¬ì˜¤ê¸°(.jsonì˜ êµ¬ì¡°ë¥¼ ì°¸ê³ í•´ì•¼ í•¨)
        metadata = {
            "category": item.get("category", ""),
            "subcategory": item.get("subcategory", ""),
            "title": item.get("title", ""),
            "url": item.get("url", ""),
        }
        documents.append(Document(page_content=content, metadata=metadata))

    return documents

## 3: Documentë¥¼ ë” ì‘ì€ documentë¡œ ë³€í™˜
def chunk_documents(documents: List[Document]) -> List[Document]:
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)
    return text_splitter.split_documents(documents)

## 4: Documentë¥¼ ë²¡í„°DBë¡œ ì €ì¥
def save_to_vector_store(documents: List[Document]) -> None:
    embeddings = HuggingFaceEmbeddings(model_name="jhgan/ko-sbert-nli")
    vector_store = FAISS.from_documents(documents, embedding=embeddings)
    vector_store.save_local("faiss_index")



############################### 2ë‹¨ê³„ : RAG ê¸°ëŠ¥ êµ¬í˜„ê³¼ ê´€ë ¨ëœ í•¨ìˆ˜ë“¤ ##########################


## ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ RAG ì²˜ë¦¬
@st.cache_data
def process_question(user_question, history_text):

    embeddings = HuggingFaceEmbeddings(model_name="jhgan/ko-sbert-nli")

    ## ë²¡í„° DB í˜¸ì¶œ
    new_db = FAISS.load_local("faiss_index", embeddings, allow_dangerous_deserialization=True)

    ## ê´€ë ¨ ë¬¸ì„œ 3ê°œë¥¼ í˜¸ì¶œí•˜ëŠ” Retriever ìƒì„±
    retriever = new_db.as_retriever(search_kwargs={"k": 3})
    ## ì‚¬ìš©ì ì§ˆë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ ê´€ë ¨ë¬¸ì„œ 3ê°œ ê²€ìƒ‰ 
    retrieve_docs : List[Document] = retriever.invoke(user_question)

    ## RAG ì²´ì¸ ì„ ì–¸
    chain = get_rag_chain()
    ## ì§ˆë¬¸ê³¼ ë¬¸ë§¥ì„ ë„£ì–´ì„œ ì²´ì¸ ê²°ê³¼ í˜¸ì¶œ
    response = chain.invoke({
        "question": user_question, 
        "context": retrieve_docs,
        "history": history_text
    })

    return response, retrieve_docs



def get_rag_chain() -> Runnable:
    template = """
    ë‹¤ìŒì˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ í™œìš©í•´ì„œ ì§ˆë¬¸ì— ë‹µë³€í•´ì¤˜
    - ì§ˆë¬¸ì— ëŒ€í•œ ì‘ë‹µì„ í•´ì¤˜
    - ê°„ê²°í•˜ê²Œ 5ì¤„ ì´ë‚´ë¡œ í•´ì¤˜
    - ê³§ë°”ë¡œ ì‘ë‹µê²°ê³¼ë¥¼ ë§í•´ì¤˜
    
    ì´ì „ëŒ€í™” : {history}

    ì»¨í…ìŠ¤íŠ¸ : {context}

    ì§ˆë¬¸: {question}

    ì‘ë‹µ:"""

    custom_rag_prompt = PromptTemplate.from_template(template)
    model = ChatGoogleGenerativeAI(model="gemini-1.5-flash")
    return custom_rag_prompt | model | StrOutputParser() # pipeë¡œ ê´€ë¦¬í•˜ì—¬ in-out ì‰½ê²Œ ë„£ì–´ì£¼ê¸°



############################### 3ë‹¨ê³„ : ì‘ë‹µê²°ê³¼ì™€ ë¬¸ì„œë¥¼ í•¨ê»˜ ë³´ë„ë¡ ë„ì™€ì£¼ëŠ” í•¨ìˆ˜ ##########################
@st.cache_data(show_spinner=False)
def natural_sort_key(s):
    return [int(text) if text.isdigit() else text for text in re.split(r'(\d+)', s)]


def main():
    if not os.path.exists("faiss_index"):
        json_file = "database/test_data.json"
        json_document = json_to_documents(json_file)
        smaller_documents = chunk_documents(json_document)
        save_to_vector_store(smaller_documents)

    st.set_page_config("ë¡œìšœë¼ë„ì„œê´€ FAQ ì±—ë´‡", layout="wide")
    st.header("ë¡œìšœë¼ë„ì„œê´€ FAQ ì±—ë´‡")
    
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []

    left_column, right_column = st.columns([1, 1])

    with left_column:
        for msg in st.session_state.chat_history:
            st.chat_message(msg["role"]).write(msg["content"])

    with right_column:
        show_notices()

    user_question = st.chat_input("ë¡œìšœë¼ ë„ì„œê´€ì— ëŒ€í•´ì„œ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”")

    # ìˆ˜ì •ëœ ì½”ë“œ (if user_question ë¸”ë¡)

    if user_question:
        st.session_state.chat_history.append({"role": "user", "content": user_question})

        with left_column:
            st.chat_message("user").write(user_question)

            # ë¡œë”© ë©”ì‹œì§€ìš© with ë¸”ë¡ë§Œ ë‚¨ê¹€
            with st.chat_message("assistant"):
                message_placeholder = st.empty()
                message_placeholder.write("ğŸ¤” ë‹µë³€ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...")
        
        # ì´ ë¶€ë¶„ì— with ë¸”ë¡ì„ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
        history_text = format_history_for_prompt(st.session_state.chat_history, window_size=8)
        response, context = process_question(user_question, history_text)

        # placeholderë¥¼ ì§ì ‘ ì—…ë°ì´íŠ¸
        with left_column:
            message_placeholder.write(response)
            
        st.session_state.chat_history.append({"role": "assistant", "content": response})


if __name__ == "__main__":
    main()